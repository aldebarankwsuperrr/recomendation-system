# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ebw5m41M6ldVAKww5LmtMVeKsmfBtntI

link data set :   https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset

# Nama : Fahrul Firmansyah <br>
# Grup : M02 <br>

# Data Loading
"""

!wget --no-check-certificate \
 https://github.com/aldebarankwsuperrr/system_recomendation/blob/main/books_data.zip?raw=true \
  -O /content/books_dataset.zip

!unzip /content/books_dataset.zip -d /content

import pandas as pd
import numpy as np

books = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')

"""# Data Understanding"""

books.info()

print ("Banyak Buku : ", len(books.ISBN.unique()))

users.info()

print ("Banyak User : ", len(users['User-ID']))

ratings.info()

print("Jumlah Rating : ", len(ratings))

"""# Univariate Exploratory Data Analysis

- User <br>
Berisi pengguna. Perhatikan bahwa ID pengguna (User-ID) telah dianonimkan dan dipetakan ke bilangan bulat. Data demografis disediakan (Lokasi, Usia) jika tersedia. Jika tidak, bidang ini berisi nilai NULL.
- Books <br>
Buku diidentifikasi dengan ISBN masing-masing. ISBN yang tidak valid telah dihapus dari set data. Selain itu, beberapa informasi berbasis konten diberikan (Judul Buku, Penulis Buku, Tahun Penerbitan, Penerbit), diperoleh dari Amazon Web Services. Perhatikan bahwa dalam kasus beberapa penulis, hanya yang pertama disediakan. URL yang tertaut ke gambar sampul juga diberikan, muncul dalam tiga rasa berbeda (Image-URL-S, Image-URL-M, Image-URL-L), yaitu kecil, sedang, besar. URL ini mengarah ke situs web Amazon.
- Ratings <br>
Berisi informasi peringkat buku. Rating (Book-Rating) bersifat eksplisit, dinyatakan dalam skala 1-10 (nilai yang lebih tinggi menunjukkan apresiasi yang lebih tinggi), atau implisit, yang dinyatakan dengan 0.
"""

ratings.describe()

"""Melihat perbandingan antara data eksplisii dan implisit"""

import matplotlib.pyplot as plt
import seaborn as sns

jenis = ['eksplisit', 'implisit']
eksplisit = len(ratings[ratings['Book-Rating'] != 0])
implisit = len(ratings[ratings['Book-Rating'] == 0])
jumlah_data = [eksplisit, implisit]

plt.bar(jenis, jumlah_data)
plt.title("Perbandingan jumlah raing implisit dengan eksplisit")
plt.xlabel("jenis")
plt.ylabel("jumlah data")
plt.show()
print ("jumlah data eksplisit :" , eksplisit)
print ("jumlah data implisitt :" , implisit)

len(ratings[ratings['Book-Rating'] != 0] )

"""# Data Preparation

mengambil data eksplisit dari dataset ratings
"""

new_rating = ratings[ratings['Book-Rating'] != 0]

"""Menghapus kolom yang tidak diperlukan <br>
Melakukan penggabungan dataset buku dengan dataset rating eksplisit <br>
Menghilangkan duplikat buku berdasarkan ISBN
"""

books = books.drop(columns=['Image-URL-S', 'Image-URL-M','Image-URL-L'])
new_books = pd.merge (books, new_rating, on='ISBN', how='left')
new_books = new_books.drop_duplicates('ISBN')
new_books

"""Memeriksa missing value"""

new_books.isnull().sum()

"""Menghilangkan missing value"""

new_books = new_books.dropna()
new_books.isnull().sum()

"""Melihat penerbit teratas"""

buku_teratas = new_books['Publisher'].value_counts().head(15)

buku_teratas

"""Mengambil data-data buku dengan penerbit teratas"""

buku_harlequin = new_books[new_books['Publisher'] == 'Harlequin']
buku_harlequin = buku_harlequin.reset_index()
buku_harlequin

"""# Content Based Filtering"""

data = buku_harlequin
data.sample(5)

"""Melakukan perhitungan idf pada data cuisine dan  Mapping array dari fitur index integer ke fitur nama"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()
 
tf.fit(data['Book-Author']) 
 
tf.get_feature_names()

"""Memasukkan data penulis buku ke model"""

tfidf_matrix = tf.fit_transform(data['Book-Author']) 
 
tfidf_matrix.shape

tfidf_matrix.todense()

"""Melihat hubungan antara buku dengan fitur-fitur penting"""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data['Book-Author']
).sample(649, axis=1).sample(5, axis=0)

"""Menggunakan cosine similarity untuk mencari korelasi"""

from sklearn.metrics.pairwise import cosine_similarity
 
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku dan Melihat similarity matrix pada setiap buku"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['Book-Title'], columns=data['Book-Title'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(10, axis=1).sample(7, axis=0)

"""Membuat fungsi untuk menampilkan rekomendasi dari teknik content based filtering"""

def resto_recommendations(book_title, similarity_data=cosine_sim_df, items=data[['Book-Title', 'Book-Author']], k=5):
   
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))
    
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(book_title, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

data[data['Book-Title'].eq('Major Comes To Texas (In Uniform) (Superromance, 915)')]

"""Melakukan pemanggilan fungsi rekomendasi"""

resto_recommendations('Major Comes To Texas (In Uniform) (Superromance, 915)')

"""# Colaborative Filtering"""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""Melakukan data preparation dengan mengambil data-data buku dengan fitur User-id, ISBN, dan Book-Rating"""

df = buku_harlequin[['User-ID', 'ISBN','Book-Rating']]
df

"""Melakukan encoding untuk fitur user"""

user_ids = df['User-ID'].unique().tolist()
print('list User-ID: ', user_ids)
 
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User-ID : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User-ID: ', user_encoded_to_user)

"""melakukan encoding untuk fitur buku"""

book_ids = df['ISBN'].unique().tolist()
 
# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
 
# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

"""Melakukan mapping pada fitur user dan buku yang sudah di encoding"""

df['user'] = df['User-ID'].map(user_to_user_encoded)
 
# Mapping placeID ke dataframe resto
df['book'] = df['ISBN'].map(book_to_book_encoded)

num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah book
num_book = len(book_encoded_to_book)
print(num_book)
 
# Mengubah rating menjadi nilai float
df['rating'] = df['Book-Rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['rating'])
 
# Nilai maksimal rating
max_rating = max(df['rating'])
 
print('Number of User: {}, Number of book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

df = df.sample(frac=1, random_state=42)
df

"""Membagi dataset menjadi training data dan testing data dengan skala 80 : 20"""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""Membuat model rekomendasi dengan membuat class turunan dari tf.keras.model"""

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""melakukan inisialisasi model dan melakukan compile model"""

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Melatih model dengan jumlah epoch 100"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""Melakukan visualisasi dari latihan model"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""melakukan uji coba model dan mulai membuat rekomendasi dengan mengambil salah satu sampel user dan menandai buku yang telah dibaca."""

df = buku_harlequin[['User-ID', 'ISBN', 'Book-Rating']]
buku_df = buku_harlequin[['ISBN', 'Book-Title', 'Book-Author']]
# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
buku_visited_by_user = df[df['User-ID'] == user_id]
print(buku_visited_by_user)
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
buku_not_visited = buku_df[~buku_df.ISBN.isin(buku_visited_by_user.ISBN.values)]['ISBN'] 
buku_not_visited = list(
    set(buku_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)
 
buku_not_visited = [[book_to_book_encoded.get(x)] for x in buku_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_buku_array = np.hstack(
    ([[user_encoder]] * len(buku_not_visited), buku_not_visited)
)

"""memberikan rekomendasi untuk user """

ratings = model.predict(user_buku_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(buku_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('book with high ratings from user')
print('----' * 8)
 
top_book_user = (
    buku_visited_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5).ISBN.values
)
 
book_df_rows = buku_df[buku_df['ISBN'].isin(top_book_user)]

print(book_df_rows[['Book-Title', 'Book-Author']])
 
print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)
 
recommended_book = buku_df[buku_df['ISBN'].isin(recommended_book_ids)]
print(recommended_book[['Book-Title','Book-Author']])

